{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Evaluation — Multi-Modal Ordinal Alzheimer's Pipeline\n",
    "\n",
    "Run the full training and evaluation pipeline on Google Colab GPU.\n",
    "\n",
    "**Experiments covered:**\n",
    "1. Unimodal MRI model\n",
    "2. Unimodal Audio model\n",
    "3. Multimodal fusion (concat, gated, attention)\n",
    "4. CORAL ordinal vs cross-entropy comparison\n",
    "5. Data fraction experiments (25%, 50%, 75%, 100%)\n",
    "6. Hyperparameter sweeps\n",
    "7. 5-fold cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "**Prerequisites:** Run notebooks 01 and 02 first to extract embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/alzheimer-research'\n",
    "\n",
    "REPO_DIR = '/content/alzheimer-research'\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/YOUR_USERNAME/alzheimer-research.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from config import Config\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB')\n",
    "\n",
    "config = Config()\n",
    "config.device = device\n",
    "config.ensure_dirs()\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(config.seed)\n",
    "np.random.seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Embeddings\n",
    "\n",
    "Load pre-extracted embeddings from Google Drive. If you haven't extracted them yet, use the synthetic data option below for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIR = os.path.join(PROJECT_DIR, 'data_embeddings')\n",
    "USE_SYNTHETIC = True  # Set to False once you have real embeddings\n",
    "\n",
    "if not USE_SYNTHETIC:\n",
    "    mri_emb_path = os.path.join(EMB_DIR, 'mri_embeddings.npz')\n",
    "    labels_path = os.path.join(EMB_DIR, 'labels.csv')\n",
    "\n",
    "    assert os.path.exists(mri_emb_path), f'MRI embeddings not found at {mri_emb_path}'\n",
    "    assert os.path.exists(labels_path), f'Labels not found at {labels_path}'\n",
    "\n",
    "    import pandas as pd\n",
    "    mri_data = np.load(mri_emb_path)\n",
    "    mri_embeddings = mri_data['embeddings'].astype(np.float32)\n",
    "    labels_df = pd.read_csv(labels_path)\n",
    "    labels = labels_df['label'].values\n",
    "\n",
    "    print(f'MRI embeddings: {mri_embeddings.shape}')\n",
    "    print(f'Label distribution:')\n",
    "    for name, idx in zip(config.class_names, range(config.num_classes)):\n",
    "        print(f'  {name}: {(labels == idx).sum()}')\n",
    "else:\n",
    "    print('Using synthetic data for pipeline testing...')\n",
    "    EMBED_DIM = 256\n",
    "    N_SAMPLES = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Experiment: Unimodal MRI Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.train_unimodal import SyntheticEmbeddingDataset, train_model, run_cross_validation\n",
    "from experiments.evaluation import plot_learning_curves, plot_confusion_matrix\n",
    "\n",
    "if USE_SYNTHETIC:\n",
    "    mri_dataset = SyntheticEmbeddingDataset(\n",
    "        n_samples=N_SAMPLES, embed_dim=EMBED_DIM, num_classes=config.num_classes\n",
    "    )\n",
    "else:\n",
    "    from experiments.train_unimodal import EmbeddingDataset\n",
    "    mri_dataset = EmbeddingDataset(mri_emb_path, labels_path, modality='mri')\n",
    "\n",
    "print(f'Dataset size: {len(mri_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/val split\n",
    "from torch.utils.data import Subset\n",
    "n_total = len(mri_dataset)\n",
    "n_train = int(0.8 * n_total)\n",
    "indices = np.random.RandomState(config.seed).permutation(n_total)\n",
    "train_set = Subset(mri_dataset, indices[:n_train])\n",
    "val_set = Subset(mri_dataset, indices[n_train:])\n",
    "\n",
    "print(f'Train: {len(train_set)}, Val: {len(val_set)}')\n",
    "\n",
    "# Train with CORAL ordinal loss\n",
    "config.num_epochs = 50\n",
    "config.early_stopping_patience = 10\n",
    "\n",
    "model_mri, head_mri, metrics_mri, history_mri = train_model(\n",
    "    train_set, val_set, config,\n",
    "    embed_dim=EMBED_DIM if USE_SYNTHETIC else mri_embeddings.shape[1],\n",
    "    loss_type='coral',\n",
    ")\n",
    "\n",
    "print(f\"\\nBest MRI Results:\")\n",
    "print(f\"  Accuracy: {metrics_mri['accuracy']:.4f}\")\n",
    "print(f\"  QWK: {metrics_mri['qwk']:.4f}\")\n",
    "print(f\"  MAE: {metrics_mri['mae']:.4f}\")\n",
    "print(f\"  Off-by-1: {metrics_mri['off_by_1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves\n",
    "plot_learning_curves(history_mri, title='Unimodal MRI - Learning Curves')\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    metrics_mri['confusion_matrix'],\n",
    "    config.class_names,\n",
    "    title='Unimodal MRI - Confusion Matrix'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment: 5-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.n_folds = 5\n",
    "config.num_epochs = 50\n",
    "\n",
    "fold_metrics_mri, avg_metrics_mri = run_cross_validation(\n",
    "    mri_dataset, config,\n",
    "    embed_dim=EMBED_DIM if USE_SYNTHETIC else mri_embeddings.shape[1],\n",
    "    loss_type='coral',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Experiment: CORAL vs Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.evaluation import run_loss_comparison\n",
    "\n",
    "loss_results = run_loss_comparison(config, embed_dim=EMBED_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment: Data Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.evaluation import run_data_fraction_experiment, plot_data_fraction_results\n",
    "\n",
    "frac_results = run_data_fraction_experiment(\n",
    "    config,\n",
    "    SyntheticEmbeddingDataset,\n",
    "    {'n_samples': N_SAMPLES, 'embed_dim': EMBED_DIM, 'seed': config.seed},\n",
    "    embed_dim=EMBED_DIM,\n",
    ")\n",
    "\n",
    "plot_data_fraction_results(frac_results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment: Multimodal Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.train_multimodal import (\n",
    "    SyntheticMultimodalDataset,\n",
    "    train_model as train_multimodal,\n",
    "    run_cross_validation as run_cv_multi,\n",
    ")\n",
    "\n",
    "multi_dataset = SyntheticMultimodalDataset(\n",
    "    n_samples=N_SAMPLES, embed_dim=EMBED_DIM, seed=config.seed\n",
    ")\n",
    "\n",
    "fusion_results = {}\n",
    "\n",
    "for fusion_type in ['concat', 'gated', 'attention']:\n",
    "    print(f'\\n{\"=\"*50}')\n",
    "    print(f'Fusion Type: {fusion_type}')\n",
    "    print(f'{\"=\"*50}')\n",
    "\n",
    "    n_total = len(multi_dataset)\n",
    "    n_train = int(0.8 * n_total)\n",
    "    indices = np.random.RandomState(config.seed).permutation(n_total)\n",
    "    train_set = Subset(multi_dataset, indices[:n_train])\n",
    "    val_set = Subset(multi_dataset, indices[n_train:])\n",
    "\n",
    "    model_f, head_f, metrics_f, history_f = train_multimodal(\n",
    "        train_set, val_set, config,\n",
    "        embed_dim=EMBED_DIM, fusion_type=fusion_type, loss_type='coral',\n",
    "    )\n",
    "\n",
    "    fusion_results[fusion_type] = metrics_f\n",
    "\n",
    "    print(f'  Accuracy: {metrics_f[\"accuracy\"]:.4f}')\n",
    "    print(f'  QWK: {metrics_f[\"qwk\"]:.4f}')\n",
    "    print(f'  MAE: {metrics_f[\"mae\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print('\\n' + '=' * 70)\n",
    "print(f'{\"Model\":<25} {\"Accuracy\":>10} {\"QWK\":>10} {\"MAE\":>10} {\"Off-by-1\":>10}')\n",
    "print('-' * 70)\n",
    "print(f'{\"Unimodal MRI\":<25} {metrics_mri[\"accuracy\"]:>10.4f} {metrics_mri[\"qwk\"]:>10.4f} {metrics_mri[\"mae\"]:>10.4f} {metrics_mri[\"off_by_1\"]:>10.4f}')\n",
    "for ft, m in fusion_results.items():\n",
    "    name = f'Multimodal ({ft})'\n",
    "    print(f'{name:<25} {m[\"accuracy\"]:>10.4f} {m[\"qwk\"]:>10.4f} {m[\"mae\"]:>10.4f} {m[\"off_by_1\"]:>10.4f}')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sweep over learning rates and embedding dims\n",
    "sweep_results = []\n",
    "\n",
    "for lr in [1e-2, 1e-3, 1e-4]:\n",
    "    for embed_dim in [128, 256]:\n",
    "        for dropout in [0.2, 0.4]:\n",
    "            config.learning_rate = lr\n",
    "            config.fusion_dropout = dropout\n",
    "            config.num_epochs = 30  # Shorter for sweep\n",
    "\n",
    "            sweep_dataset = SyntheticEmbeddingDataset(\n",
    "                n_samples=N_SAMPLES, embed_dim=embed_dim, seed=config.seed\n",
    "            )\n",
    "            n_total = len(sweep_dataset)\n",
    "            n_train = int(0.8 * n_total)\n",
    "            indices = np.random.RandomState(config.seed).permutation(n_total)\n",
    "            train_set = Subset(sweep_dataset, indices[:n_train])\n",
    "            val_set = Subset(sweep_dataset, indices[n_train:])\n",
    "\n",
    "            _, _, metrics, _ = train_model(\n",
    "                train_set, val_set, config,\n",
    "                embed_dim=embed_dim, loss_type='coral', verbose=False,\n",
    "            )\n",
    "\n",
    "            result = {\n",
    "                'lr': lr, 'embed_dim': embed_dim, 'dropout': dropout,\n",
    "                'accuracy': metrics['accuracy'], 'qwk': metrics['qwk'],\n",
    "                'mae': metrics['mae'],\n",
    "            }\n",
    "            sweep_results.append(result)\n",
    "            print(f'lr={lr}, dim={embed_dim}, dropout={dropout} → '\n",
    "                  f'Acc={metrics[\"accuracy\"]:.4f}, QWK={metrics[\"qwk\"]:.4f}')\n",
    "\n",
    "# Find best config\n",
    "best = max(sweep_results, key=lambda x: x['qwk'])\n",
    "print(f'\\nBest config: lr={best[\"lr\"]}, dim={best[\"embed_dim\"]}, '\n",
    "      f'dropout={best[\"dropout\"]} → QWK={best[\"qwk\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RESULTS_DIR = os.path.join(PROJECT_DIR, 'experiment_results')\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Save sweep results\n",
    "sweep_df = pd.DataFrame(sweep_results)\n",
    "sweep_df.to_csv(os.path.join(RESULTS_DIR, 'hyperparameter_sweep.csv'), index=False)\n",
    "\n",
    "# Save model comparison\n",
    "comparison_rows = []\n",
    "comparison_rows.append({'model': 'Unimodal MRI', **{k: metrics_mri[k] for k in ['accuracy', 'qwk', 'mae', 'off_by_1']}})\n",
    "for ft, m in fusion_results.items():\n",
    "    comparison_rows.append({'model': f'Multimodal ({ft})', **{k: m[k] for k in ['accuracy', 'qwk', 'mae', 'off_by_1']}})\n",
    "comp_df = pd.DataFrame(comparison_rows)\n",
    "comp_df.to_csv(os.path.join(RESULTS_DIR, 'model_comparison.csv'), index=False)\n",
    "\n",
    "print('Results saved to Google Drive:')\n",
    "print(f'  {RESULTS_DIR}/hyperparameter_sweep.csv')\n",
    "print(f'  {RESULTS_DIR}/model_comparison.csv')\n",
    "print('\\nDone!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
