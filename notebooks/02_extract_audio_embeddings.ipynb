{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Embedding Extraction\n",
    "\n",
    "This notebook extracts embeddings from speech/audio data for Alzheimer's detection.\n",
    "\n",
    "**Supported audio sources:**\n",
    "- ADReSS / ADReSSo challenge datasets\n",
    "- DementiaBank Pitt Corpus\n",
    "- Any WAV files organized by class folders\n",
    "\n",
    "**Pipeline:** WAV → Log-Mel Spectrogram → ResNet-18 → 256-D Embedding → Compressed .npz\n",
    "\n",
    "---\n",
    "\n",
    "**Instructions:**\n",
    "1. Select GPU runtime: Runtime → Change runtime type → T4 GPU\n",
    "2. Upload or download your audio dataset\n",
    "3. Run all cells\n",
    "4. Embeddings saved to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/alzheimer-research'\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "\n",
    "REPO_DIR = '/content/alzheimer-research'\n",
    "if not os.path.exists(REPO_DIR):\n",
    "    !git clone https://github.com/YOUR_USERNAME/alzheimer-research.git {REPO_DIR}\n",
    "else:\n",
    "    !cd {REPO_DIR} && git pull\n",
    "\n",
    "os.chdir(REPO_DIR)\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, REPO_DIR)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Data Source\n",
    "\n",
    "Organize your audio files into class folders:\n",
    "```\n",
    "audio_data/\n",
    "  NonDemented/\n",
    "    file1.wav\n",
    "    file2.wav\n",
    "  VeryMildDemented/\n",
    "    ...\n",
    "  MildDemented/\n",
    "    ...\n",
    "  ModerateDemented/\n",
    "    ...\n",
    "```\n",
    "\n",
    "If your dataset uses different class names (e.g., `cc` and `cd` for ADReSS),\n",
    "update the `LABEL_MAP` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_DATA_DIR = '/tmp/audio_data'  # Update this path\n",
    "\n",
    "# Label mapping: folder name -> ordinal class index\n",
    "# Default: 4-class CDR staging\n",
    "LABEL_MAP = {\n",
    "    'NonDemented': 0,\n",
    "    'VeryMildDemented': 1,\n",
    "    'MildDemented': 2,\n",
    "    'ModerateDemented': 3,\n",
    "}\n",
    "\n",
    "# Alternative for ADReSS (binary classification → adapt for ordinal):\n",
    "# LABEL_MAP = {\n",
    "#     'cc': 0,  # Control (non-demented)\n",
    "#     'cd': 2,  # Dementia (map to MildDemented)\n",
    "# }\n",
    "\n",
    "# Audio parameters\n",
    "TARGET_SAMPLE_RATE = 16000\n",
    "MAX_AUDIO_LENGTH = TARGET_SAMPLE_RATE * 10  # 10 seconds max\n",
    "EMBED_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.audio_cnn import AudioCNN\n",
    "\n",
    "model = AudioCNN(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    pretrained=True,\n",
    "    sample_rate=TARGET_SAMPLE_RATE,\n",
    "    n_mels=128,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    from_spectrogram=False,  # Input is raw waveform\n",
    ").to(device)\n",
    "model.eval()\n",
    "print(f'AudioCNN loaded (embed_dim={EMBED_DIM})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_audio(filepath, target_sr=16000, max_length=None):\n",
    "    \"\"\"Load audio file, resample, and pad/truncate to fixed length.\"\"\"\n",
    "    waveform, sr = torchaudio.load(filepath)\n",
    "\n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sr != target_sr:\n",
    "        resampler = torchaudio.transforms.Resample(sr, target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    # Pad or truncate\n",
    "    if max_length is not None:\n",
    "        if waveform.shape[1] > max_length:\n",
    "            waveform = waveform[:, :max_length]\n",
    "        elif waveform.shape[1] < max_length:\n",
    "            padding = max_length - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
    "\n",
    "    return waveform  # (1, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract audio embeddings\n",
    "all_embeddings = []\n",
    "all_labels = []\n",
    "all_filenames = []\n",
    "\n",
    "for class_name, label in LABEL_MAP.items():\n",
    "    class_dir = os.path.join(AUDIO_DATA_DIR, class_name)\n",
    "    if not os.path.exists(class_dir):\n",
    "        print(f'Skipping {class_name}: directory not found')\n",
    "        continue\n",
    "\n",
    "    audio_files = [f for f in sorted(os.listdir(class_dir))\n",
    "                   if f.endswith(('.wav', '.mp3', '.flac', '.ogg'))]\n",
    "    print(f'Processing {class_name} ({len(audio_files)} files)...')\n",
    "\n",
    "    for fname in tqdm(audio_files, desc=class_name):\n",
    "        fpath = os.path.join(class_dir, fname)\n",
    "        try:\n",
    "            waveform = load_and_preprocess_audio(\n",
    "                fpath, TARGET_SAMPLE_RATE, MAX_AUDIO_LENGTH\n",
    "            )\n",
    "            # Add batch dim: (1, 1, T)\n",
    "            waveform = waveform.unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                emb = model.extract_embedding(waveform)\n",
    "\n",
    "            all_embeddings.append(emb.cpu().numpy())\n",
    "            all_labels.append(label)\n",
    "            all_filenames.append(fname)\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {fname}: {e}')\n",
    "\n",
    "if all_embeddings:\n",
    "    embeddings = np.concatenate(all_embeddings, axis=0)\n",
    "    labels = np.array(all_labels)\n",
    "    print(f'\\nExtracted {len(embeddings)} audio embeddings of dimension {embeddings.shape[1]}')\n",
    "else:\n",
    "    print('No audio files processed. Check your data directory and LABEL_MAP.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save compressed embeddings to Google Drive\n",
    "import pandas as pd\n",
    "\n",
    "SAVE_DIR = os.path.join(PROJECT_DIR, 'data_embeddings')\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "if all_embeddings:\n",
    "    emb_path = os.path.join(SAVE_DIR, 'audio_embeddings.npz')\n",
    "    np.savez_compressed(emb_path, embeddings=embeddings.astype(np.float16))\n",
    "    print(f'Saved audio embeddings to {emb_path}')\n",
    "    print(f'File size: {os.path.getsize(emb_path) / 1024 / 1024:.2f} MB')\n",
    "\n",
    "    # Save labels (or append to existing)\n",
    "    labels_path = os.path.join(SAVE_DIR, 'audio_labels.csv')\n",
    "    df = pd.DataFrame({\n",
    "        'filename': all_filenames,\n",
    "        'label': labels,\n",
    "        'class_name': [list(LABEL_MAP.keys())[l] for l in labels],\n",
    "    })\n",
    "    df.to_csv(labels_path, index=False)\n",
    "    print(f'Saved labels to {labels_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "if os.path.exists(AUDIO_DATA_DIR):\n",
    "    shutil.rmtree(AUDIO_DATA_DIR)\n",
    "    print(f'Cleaned up {AUDIO_DATA_DIR}')\n",
    "\n",
    "print('Done! Audio embeddings saved to Google Drive.')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
