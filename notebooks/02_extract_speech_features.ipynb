{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 — Extract Speech Biomarkers from DementiaBank\n",
    "\n",
    "Extracts four feature streams per recording:\n",
    "1. Handcrafted acoustic features (eGeMAPS-style) → 216-D\n",
    "2. wav2vec 2.0 embeddings → 768-D\n",
    "3. Handcrafted linguistic features → 14-D\n",
    "4. Sentence-BERT transcript embeddings → 384-D\n",
    "\n",
    "Total: 1382-D per recording, saved as .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "# !pip install transformers librosa parselmouth sentence-transformers -q\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/content/drive/MyDrive/alzheimer-research')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from config import Config\n",
    "cfg = Config()\n",
    "cfg.ensure_dirs()\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure DementiaBank Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURE THESE ===\n",
    "DEMENTIABANK_DIR = Path('path/to/DementiaBank/Pitt')\n",
    "TRANSCRIPT_DIR = DEMENTIABANK_DIR / 'transcripts'   # .cha files or .txt\n",
    "AUDIO_DIR = DEMENTIABANK_DIR / 'audio'              # .wav files\n",
    "OUTPUT_PATH = cfg.embedding_dir / 'speech_features.npz'\n",
    "\n",
    "# Expected structure:\n",
    "# DementiaBank/Pitt/\n",
    "#   Dementia/\n",
    "#     *.wav  (or .mp3)\n",
    "#   Control/\n",
    "#     *.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Handcrafted Acoustic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preprocessing import extract_acoustic_handcrafted\n",
    "\n",
    "# Collect audio files and labels\n",
    "audio_files = []\n",
    "labels = []\n",
    "\n",
    "# Adapt this to your DementiaBank directory structure\n",
    "# for wav in sorted(AUDIO_DIR.glob('Dementia/**/*.wav')):\n",
    "#     audio_files.append(wav)\n",
    "#     labels.append(2)  # map to ordinal class based on MMSE/diagnosis\n",
    "# for wav in sorted(AUDIO_DIR.glob('Control/**/*.wav')):\n",
    "#     audio_files.append(wav)\n",
    "#     labels.append(0)\n",
    "\n",
    "print(f'Total audio files: {len(audio_files)}')\n",
    "\n",
    "# Extract\n",
    "acoustic_features = []\n",
    "for path in tqdm(audio_files, desc='Acoustic features'):\n",
    "    feat = extract_acoustic_handcrafted(path)\n",
    "    acoustic_features.append(feat)\n",
    "\n",
    "acoustic_features = np.stack(acoustic_features) if acoustic_features else np.zeros((0, 216))\n",
    "print(f'Acoustic features shape: {acoustic_features.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract wav2vec 2.0 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "import librosa\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained('facebook/wav2vec2-base')\n",
    "wav2vec_model = Wav2Vec2Model.from_pretrained('facebook/wav2vec2-base').to(DEVICE)\n",
    "wav2vec_model.eval()\n",
    "\n",
    "wav2vec_embeds = []\n",
    "for path in tqdm(audio_files, desc='wav2vec2'):\n",
    "    audio, sr = librosa.load(str(path), sr=16000, mono=True)\n",
    "    inputs = processor(audio, sampling_rate=16000, return_tensors='pt', padding=True)\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = wav2vec_model(**inputs)\n",
    "        # Mean-pool over time dimension\n",
    "        embed = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    wav2vec_embeds.append(embed)\n",
    "\n",
    "wav2vec_embeds = np.stack(wav2vec_embeds) if wav2vec_embeds else np.zeros((0, 768))\n",
    "print(f'wav2vec2 embeddings shape: {wav2vec_embeds.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.preprocessing import extract_linguistic_handcrafted\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load transcripts (adapt to your format: .cha, .txt, or Whisper ASR)\n",
    "transcripts = []\n",
    "# for path in audio_files:\n",
    "#     transcript_path = TRANSCRIPT_DIR / (path.stem + '.txt')\n",
    "#     if transcript_path.exists():\n",
    "#         transcripts.append(transcript_path.read_text())\n",
    "#     else:\n",
    "#         # Use Whisper ASR as fallback\n",
    "#         transcripts.append('')  # placeholder\n",
    "\n",
    "# Handcrafted linguistic\n",
    "ling_features = []\n",
    "for text in tqdm(transcripts, desc='Linguistic features'):\n",
    "    feat = extract_linguistic_handcrafted(text)\n",
    "    ling_features.append(feat)\n",
    "ling_features = np.stack(ling_features) if ling_features else np.zeros((0, 14))\n",
    "\n",
    "# Sentence-BERT embeddings\n",
    "sbert = SentenceTransformer('all-MiniLM-L6-v2', device=str(DEVICE))\n",
    "sbert_embeds = sbert.encode(transcripts, show_progress_bar=True) if transcripts else np.zeros((0, 384))\n",
    "print(f'Linguistic features: {ling_features.shape}')\n",
    "print(f'Sentence-BERT embeddings: {sbert_embeds.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Concatenate and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all feature streams: [acoustic(216) | wav2vec(768) | linguistic(14) | sbert(384)]\n",
    "if len(audio_files) > 0:\n",
    "    all_features = np.concatenate(\n",
    "        [acoustic_features, wav2vec_embeds, ling_features, sbert_embeds],\n",
    "        axis=1,\n",
    "    )\n",
    "    all_labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "    print(f'Combined features shape: {all_features.shape}')  # (N, 1382)\n",
    "    print(f'Labels shape: {all_labels.shape}')\n",
    "\n",
    "    np.savez_compressed(\n",
    "        str(OUTPUT_PATH),\n",
    "        features=all_features.astype(np.float32),\n",
    "        labels=all_labels,\n",
    "    )\n",
    "    print(f'Saved to {OUTPUT_PATH}')\n",
    "else:\n",
    "    print('No audio files found — configure DEMENTIABANK_DIR above')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
